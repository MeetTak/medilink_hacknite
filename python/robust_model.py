import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import pickle
import re

print("ðŸš€ Creating a robust drug prediction model...")

# Load the dataset
print("ðŸ“Š Loading dataset...")
df = pd.read_csv('drugs_side_effects_drugs_com.csv')
print(f"âœ… Dataset loaded successfully! Shape: {df.shape}")

# Clean the data
print("ðŸ§¹ Cleaning and preparing data...")

def clean_text(text):
    """Clean side effects text"""
    if pd.isna(text):
        return ""
    text = str(text).lower()
    # Remove special characters but keep spaces
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)
    # Remove extra whitespace
    text = ' '.join(text.split())
    return text

# Filter and clean data
df_clean = df[['drug_name', 'side_effects']].copy()
df_clean = df_clean.dropna()
print(f"âœ… After removing missing values: {len(df_clean)} rows")

# Clean the text data
df_clean['side_effects_clean'] = df_clean['side_effects'].apply(clean_text)
df_clean['drug_name_clean'] = df_clean['drug_name'].apply(lambda x: str(x).strip().lower())

# Remove entries with very short side effects
df_clean = df_clean[df_clean['side_effects_clean'].str.len() >= 50]
print(f"âœ… After removing short side effects: {len(df_clean)} rows")

# Filter drugs that appear at least 2 times for proper train/test split
drug_counts = df_clean['drug_name_clean'].value_counts()
drugs_with_multiple_entries = drug_counts[drug_counts >= 2].index

# Filter the dataset
df_filtered = df_clean[df_clean['drug_name_clean'].isin(drugs_with_multiple_entries)]
print(f"âœ… After filtering (drugs with 2+ entries): {len(df_filtered)} rows, {df_filtered['drug_name_clean'].nunique()} unique drugs")

if len(df_filtered) < 20:
    print("âŒ Not enough data for training. Let's use a different approach...")
    
    # Alternative approach: Create synthetic samples by using medical condition mapping
    # Use the original dataset but map similar conditions
    print("ðŸ”„ Using alternative approach with condition-based grouping...")
    
    # Load a simpler approach - use the drugs.json data
    import json
    try:
        with open('../src/main/resources/data/drugs.json', 'r') as f:
            drugs_data = json.load(f)
        
        print(f"ðŸ“Š Loaded {len(drugs_data)} drugs from JSON file")
        
        # Create a simple mapping from common symptoms to drugs
        symptom_drug_mapping = {}
        
        for drug in drugs_data:
            drug_name = drug.get('name', '').lower()
            drug_class = drug.get('drugClass', '').lower()
            
            # Simple symptom-to-drug mapping based on drug classes
            if 'antibiotic' in drug_class or 'antimicrobial' in drug_class:
                for symptom in ['infection', 'fever', 'bacteria', 'sepsis']:
                    if symptom not in symptom_drug_mapping:
                        symptom_drug_mapping[symptom] = []
                    symptom_drug_mapping[symptom].append(drug_name)
            
            elif 'analgesic' in drug_class or 'pain' in drug_class:
                for symptom in ['pain', 'headache', 'muscle ache', 'arthritis']:
                    if symptom not in symptom_drug_mapping:
                        symptom_drug_mapping[symptom] = []
                    symptom_drug_mapping[symptom].append(drug_name)
            
            elif 'cardiovascular' in drug_class or 'heart' in drug_class:
                for symptom in ['chest pain', 'hypertension', 'heart disease']:
                    if symptom not in symptom_drug_mapping:
                        symptom_drug_mapping[symptom] = []
                    symptom_drug_mapping[symptom].append(drug_name)
        
        # Create training data from the mapping
        training_data = []
        for symptom, drugs in symptom_drug_mapping.items():
            for drug in drugs[:5]:  # Limit to 5 drugs per symptom
                training_data.append({
                    'side_effects_clean': symptom,
                    'drug_name_clean': drug
                })
        
        df_synthetic = pd.DataFrame(training_data)
        print(f"âœ… Created synthetic dataset: {len(df_synthetic)} samples, {df_synthetic['drug_name_clean'].nunique()} unique drugs")
        
        # Use synthetic data
        X = df_synthetic['side_effects_clean']
        y = df_synthetic['drug_name_clean']
        
    except Exception as e:
        print(f"âŒ Could not load JSON data: {e}")
        print("ðŸ”„ Creating minimal functional model...")
        
        # Create a minimal functional model with basic symptom-drug pairs
        basic_data = [
            ('headache pain migraine', 'ibuprofen'),
            ('fever temperature high', 'acetaminophen'),
            ('infection bacterial', 'amoxicillin'),
            ('allergy allergic reaction', 'diphenhydramine'),
            ('stomach pain nausea', 'omeprazole'),
            ('cough cold respiratory', 'dextromethorphan'),
            ('skin rash itching', 'hydrocortisone'),
            ('anxiety stress nervous', 'lorazepam'),
            ('depression mood sad', 'sertraline'),
            ('diabetes blood sugar', 'metformin'),
            ('heart chest pain', 'atenolol'),
            ('blood pressure hypertension', 'lisinopril'),
            ('cholesterol lipid', 'atorvastatin'),
            ('asthma breathing difficulty', 'albuterol'),
            ('arthritis joint pain', 'naproxen')
        ]
        
        df_basic = pd.DataFrame(basic_data, columns=['side_effects_clean', 'drug_name_clean'])
        
        # Duplicate entries to have enough for train/test split
        df_expanded = pd.concat([df_basic] * 10, ignore_index=True)
        
        X = df_expanded['side_effects_clean']
        y = df_expanded['drug_name_clean']
        
        print(f"âœ… Created basic model dataset: {len(X)} samples, {len(y.unique())} unique drugs")

else:
    # Use the filtered dataset
    X = df_filtered['side_effects_clean']
    y = df_filtered['drug_name_clean']

print(f"ðŸ”€ Final dataset: {len(X)} samples, {len(y.unique())} unique drugs")

# Split the data without stratification to avoid the error
print("ðŸ”€ Splitting data...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

print(f"ðŸ“ˆ Training set: {len(X_train)} samples")
print(f"ðŸ“‰ Test set: {len(X_test)} samples")

# Create TF-IDF vectorizer
print("ðŸ”§ Creating TF-IDF vectorizer...")
vectorizer = TfidfVectorizer(
    max_features=1000,
    stop_words='english',
    ngram_range=(1, 2),
    min_df=1,
    max_df=0.9
)

# Fit and transform training data
print("ðŸ“Š Vectorizing text data...")
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

print(f"âœ… Feature matrix shape: {X_train_tfidf.shape}")

# Train the model
print("ðŸ¤– Training Multinomial Naive Bayes model...")
model = MultinomialNB(alpha=1.0)
model.fit(X_train_tfidf, y_train)

# Make predictions
print("ðŸ”® Making predictions...")
y_pred = model.predict(X_test_tfidf)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"âœ… Model accuracy: {accuracy:.4f}")

# Test with example symptoms
print("\nðŸ§ª Testing with example symptoms:")
test_symptoms = [
    "headache pain",
    "fever high temperature",
    "infection bacterial",
    "allergy allergic reaction",
    "stomach pain nausea",
    "chest pain heart",
    "skin rash itching"
]

for symptom in test_symptoms:
    symptom_tfidf = vectorizer.transform([symptom])
    predicted_drug = model.predict(symptom_tfidf)[0]
    prediction_proba = model.predict_proba(symptom_tfidf)[0]
    max_prob = max(prediction_proba)
    
    print(f"Symptoms: '{symptom}' â†’ {predicted_drug} (confidence: {max_prob:.2f})")

# Save the model
print("\nðŸ’¾ Saving the model...")
model_data = {
    'model': model,
    'vectorizer': vectorizer,
    'accuracy': accuracy,
    'classes': model.classes_
}

with open('drug_prediction_model.pkl', 'wb') as f:
    pickle.dump(model_data, f)

print("âœ… Model saved successfully!")
print(f"ðŸ“Š Model stats: {len(X_train)} training samples, {accuracy:.4f} accuracy, {len(model.classes_)} drugs")

# Also copy to the webb directory
import shutil
shutil.copy('drug_prediction_model.pkl', '../hacka_boot_data/webb/drug_prediction_model.pkl')
print("âœ… Model also copied to webb directory!")
